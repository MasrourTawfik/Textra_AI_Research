{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Reinforcement Learning Papers Extraction\n",
    "This notebook implements the paper extraction pipeline for the TEXTRA-AI project, focusing on RL papers from ArXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (4.67.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from pytesseract) (24.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from pytesseract) (10.3.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thinkpad\\anaconda3\\envs\\dsvis\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv pytesseract pdf2image pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook qtconsole run script server\n",
      "troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ raw_pdfs: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\n",
      "✓ metadata: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\metadata\n",
      "✓ processed_text: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\processed\\text\n",
      "✓ vectors: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\processed\\vectors\n",
      "✓ knowledge_base: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\knowledge_base\n"
     ]
    }
   ],
   "source": [
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(project_root,\"data\")\n",
    "\n",
    "dirs = {\n",
    "'raw_pdfs': os.path.join(data_dir, \"raw\", \"pdfs\"),\n",
    "'metadata': os.path.join(data_dir, \"raw\", \"metadata\"),\n",
    "'processed_text': os.path.join(data_dir, \"processed\", \"text\"),\n",
    "'vectors': os.path.join(data_dir, \"processed\", \"vectors\"),\n",
    "'knowledge_base': os.path.join(data_dir, \"knowledge_base\")\n",
    "}\n",
    "\n",
    "for name, path in dirs.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"✓ {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Paper Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_17508\\2352384537.py:17: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n",
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=reinforcement+learning&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching ArXiv for: reinforcement learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:arxiv:Got first page: 100 of 347227 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 papers\n",
      "\n",
      "Found papers:\n",
      "\n",
      "1. Title: Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?\n",
      "   Authors: Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst\n",
      "   Published: 2024-11-06\n",
      "\n",
      "2. Title: Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation\n",
      "   Authors: Shreya Gummadi, Mateus V. Gasparino, Deepak Vasisht, Girish Chowdhary\n",
      "   Published: 2024-11-06\n",
      "\n",
      "3. Title: Self-Consistency Preference Optimization\n",
      "   Authors: Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, Jane Yu\n",
      "   Published: 2024-11-06\n",
      "\n",
      "4. Title: Weighted Sobolev Approximation Rates for Neural Networks on Unbounded Domains\n",
      "   Authors: Ahmed Abdeljawad, Thomas Dittrich\n",
      "   Published: 2024-11-06\n",
      "\n",
      "5. Title: A Comparative Study of Deep Reinforcement Learning for Crop Production Management\n",
      "   Authors: Joseph Balderas, Dong Chen, Yanbo Huang, Li Wang, Ren-Cang Li\n",
      "   Published: 2024-11-06\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Test improved search\n",
    "def search_papers(max_results: int = 10) -> List[arxiv.Result]:\n",
    "    \"\"\"Search for RL papers on ArXiv\"\"\"\n",
    "    \n",
    "    # Simplified query that will definitely return RL papers\n",
    "    query = \"reinforcement learning\"\n",
    "    \n",
    "    print(f\"Searching ArXiv for: {query}\")\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = list(search.results())\n",
    "        print(f\"Found {len(results)} papers\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")\n",
    "        return []\n",
    "\n",
    "# Try the search\n",
    "results = search_papers(max_results=5)\n",
    "\n",
    "# Verify results\n",
    "if results:\n",
    "    print(\"\\nFound papers:\")\n",
    "    for i, paper in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Title: {paper.title}\")\n",
    "        print(f\"   Authors: {', '.join(author.name for author in paper.authors)}\")\n",
    "        print(f\"   Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(\"No results found. Please check your internet connection and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Examine Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display basic information about found papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?\n",
      "Authors: Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst\n",
      "Published: 2024-11-06\n",
      "ArXiv ID: 2411.04118v1\n",
      "Categories: cs.CL, cs.AI, cs.LG\n",
      "RL-related: No\n",
      "\n",
      "Abstract snippet: Several recent works seek to develop foundation models specifically for\n",
      "medical applications, adapting general-purpose large language models (LLMs) and\n",
      "vision-language models (VLMs) via continued pret...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation\n",
      "Authors: Shreya Gummadi, Mateus V. Gasparino, Deepak Vasisht, Girish Chowdhary\n",
      "Published: 2024-11-06\n",
      "ArXiv ID: 2411.04112v1\n",
      "Categories: cs.RO, cs.AI, cs.CV, cs.DC\n",
      "RL-related: No\n",
      "\n",
      "Abstract snippet: Centralized learning requires data to be aggregated at a central server,\n",
      "which poses significant challenges in terms of data privacy and bandwidth\n",
      "consumption. Federated learning presents a compelling...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Self-Consistency Preference Optimization\n",
      "Authors: Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, Jane Yu\n",
      "Published: 2024-11-06\n",
      "ArXiv ID: 2411.04109v1\n",
      "Categories: cs.CL, cs.AI, cs.LG\n",
      "RL-related: No\n",
      "\n",
      "Abstract snippet: Self-alignment, whereby models learn to improve themselves without human\n",
      "annotation, is a rapidly growing research area. However, existing techniques\n",
      "often fail to improve complex reasoning tasks due ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Weighted Sobolev Approximation Rates for Neural Networks on Unbounded Domains\n",
      "Authors: Ahmed Abdeljawad, Thomas Dittrich\n",
      "Published: 2024-11-06\n",
      "ArXiv ID: 2411.04108v1\n",
      "Categories: cs.LG, math.FA, stat.ML, 41A25, 41A46, 41A30, 46E35, 62M45, 68T05\n",
      "RL-related: No\n",
      "\n",
      "Abstract snippet: In this work, we consider the approximation capabilities of shallow neural\n",
      "networks in weighted Sobolev spaces for functions in the spectral Barron space.\n",
      "The existing literature already covers severa...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: A Comparative Study of Deep Reinforcement Learning for Crop Production Management\n",
      "Authors: Joseph Balderas, Dong Chen, Yanbo Huang, Li Wang, Ren-Cang Li\n",
      "Published: 2024-11-06\n",
      "ArXiv ID: 2411.04106v1\n",
      "Categories: eess.SY, cs.LG, cs.SY\n",
      "RL-related: Yes\n",
      "\n",
      "Abstract snippet: Crop production management is essential for optimizing yield and minimizing a\n",
      "field's environmental impact to crop fields, yet it remains challenging due to\n",
      "the complex and stochastic processes involv...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display information about found papers and verify RL content\n",
    "for paper in results:\n",
    "    # Check if paper is RL-related\n",
    "    is_rl = ('reinforcement learning' in paper.title.lower() or \n",
    "             'reinforcement learning' in paper.summary.lower())\n",
    "    \n",
    "    print(f\"\\nTitle: {paper.title}\")\n",
    "    print(f\"Authors: {', '.join(author.name for author in paper.authors)}\")\n",
    "    print(f\"Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"ArXiv ID: {paper.get_short_id()}\")\n",
    "    print(f\"Categories: {', '.join(paper.categories)}\")\n",
    "    print(f\"RL-related: {'Yes' if is_rl else 'No'}\")\n",
    "    print(f\"\\nAbstract snippet: {paper.summary[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Download Papers Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_papers(results: List[arxiv.Result]) -> List[Dict]:\n",
    "    \"\"\"Download papers and save metadata\"\"\"\n",
    "    metadata_list = []\n",
    "    \n",
    "    print(f\"Downloading {len(results)} papers...\")\n",
    "    for paper in results:\n",
    "        try:\n",
    "            # Create metadata\n",
    "            metadata = {\n",
    "                'title': paper.title,\n",
    "                'authors': [author.name for author in paper.authors],\n",
    "                'published': paper.published.strftime('%Y-%m-%d'),\n",
    "                'arxiv_id': paper.get_short_id(),\n",
    "                'pdf_url': paper.pdf_url,\n",
    "                'abstract': paper.summary,\n",
    "                'categories': paper.categories\n",
    "            }\n",
    "            \n",
    "            # Fix the PDF path construction\n",
    "            pdf_filename = f\"{paper.get_short_id()}.pdf\"  # Just the filename\n",
    "            pdf_path = os.path.join(dirs['raw_pdfs'], pdf_filename)  # Correct path joining\n",
    "            \n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"Downloading: {paper.title}\")\n",
    "                try:\n",
    "                    # Download the PDF directly using requests\n",
    "                    response = requests.get(paper.pdf_url)\n",
    "                    response.raise_for_status()  # Check for download errors\n",
    "                    \n",
    "                    # Save the PDF\n",
    "                    with open(pdf_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"Successfully downloaded to: {pdf_path}\")\n",
    "                    time.sleep(1)  # Be nice to ArXiv servers\n",
    "                except Exception as download_error:\n",
    "                    print(f\"Download error for {paper.title}: {download_error}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Already exists: {paper.title}\")\n",
    "            \n",
    "            metadata['local_pdf_path'] = pdf_path\n",
    "            metadata_list.append(metadata)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing paper {paper.get_short_id()}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"Download complete!\")\n",
    "    return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_17508\\2352384537.py:17: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n",
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=reinforcement+learning&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching ArXiv for: reinforcement learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:arxiv:Got first page: 100 of 347227 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 papers\n",
      "Downloading 5 papers...\n",
      "Downloading: Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?\n",
      "Successfully downloaded to: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04118v1.pdf\n",
      "Downloading: Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation\n",
      "Successfully downloaded to: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04112v1.pdf\n",
      "Downloading: Self-Consistency Preference Optimization\n",
      "Successfully downloaded to: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04109v1.pdf\n",
      "Downloading: Weighted Sobolev Approximation Rates for Neural Networks on Unbounded Domains\n",
      "Successfully downloaded to: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04108v1.pdf\n",
      "Downloading: A Comparative Study of Deep Reinforcement Learning for Crop Production Management\n",
      "Successfully downloaded to: c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04106v1.pdf\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "results = search_papers(max_results=5)\n",
    "metadata_list = download_papers(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Save and Display Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           5 non-null      object\n",
      " 1   authors         5 non-null      object\n",
      " 2   published       5 non-null      object\n",
      " 3   arxiv_id        5 non-null      object\n",
      " 4   pdf_url         5 non-null      object\n",
      " 5   abstract        5 non-null      object\n",
      " 6   categories      5 non-null      object\n",
      " 7   local_pdf_path  5 non-null      object\n",
      "dtypes: object(8)\n",
      "memory usage: 448.0+ bytes\n",
      "None\n",
      "\n",
      "Downloaded papers:\n",
      "                                                                                                    title   published                                                                        local_pdf_path\n",
      "0                Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?  2024-11-06  c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04118v1.pdf\n",
      "1  Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation  2024-11-06  c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04112v1.pdf\n",
      "2                                                                Self-Consistency Preference Optimization  2024-11-06  c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04109v1.pdf\n",
      "3                           Weighted Sobolev Approximation Rates for Neural Networks on Unbounded Domains  2024-11-06  c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04108v1.pdf\n",
      "4                       A Comparative Study of Deep Reinforcement Learning for Crop Production Management  2024-11-06  c:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04106v1.pdf\n"
     ]
    }
   ],
   "source": [
    "# Save metadata\n",
    "if metadata_list:\n",
    "    metadata_df = pd.DataFrame(metadata_list)\n",
    "    metadata_path = os.path.join(dirs['metadata'], 'papers_metadata.csv')\n",
    "    metadata_df.to_csv(metadata_path, index=False)\n",
    "    \n",
    "    print(\"\\nMetadata Summary:\")\n",
    "    print(metadata_df.info())\n",
    "    print(\"\\nDownloaded papers:\")\n",
    "    print(metadata_df[['title', 'published', 'local_pdf_path']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "metadata_df['year'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Number of Papers by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
