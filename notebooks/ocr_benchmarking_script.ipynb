{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import psutil\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:11:16,072 - INFO - Processing 2411.04106v1.pdf\n",
      "2024-11-07 16:11:16,073 - INFO - Converting PDF to images: C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04106v1.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDF files in directory\n",
      "Starting benchmark process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:11:19,586 - INFO - Successfully converted 10 pages from C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04106v1.pdf\n",
      "2024-11-07 16:11:26,910 - INFO - Processing 2411.04108v1.pdf\n",
      "2024-11-07 16:11:26,911 - INFO - Converting PDF to images: C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04108v1.pdf\n",
      "2024-11-07 16:11:36,322 - INFO - Successfully converted 29 pages from C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04108v1.pdf\n",
      "2024-11-07 16:11:41,101 - INFO - Processing 2411.04109v1.pdf\n",
      "2024-11-07 16:11:41,102 - INFO - Converting PDF to images: C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04109v1.pdf\n",
      "2024-11-07 16:11:45,536 - INFO - Successfully converted 16 pages from C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04109v1.pdf\n",
      "2024-11-07 16:11:51,409 - INFO - Processing 2411.04112v1.pdf\n",
      "2024-11-07 16:11:51,410 - INFO - Converting PDF to images: C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04112v1.pdf\n",
      "2024-11-07 16:11:53,558 - INFO - Successfully converted 8 pages from C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04112v1.pdf\n",
      "2024-11-07 16:12:01,001 - INFO - Processing 2411.04118v1.pdf\n",
      "2024-11-07 16:12:01,002 - INFO - Converting PDF to images: C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04118v1.pdf\n",
      "2024-11-07 16:12:13,547 - INFO - Successfully converted 28 pages from C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs\\2411.04118v1.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark complete. Results saved to 'tesseract_benchmark_results.csv'\n",
      "Detailed report saved to 'tesseract_benchmark_report.json'\n",
      "\n",
      "Summary Statistics:\n",
      "Files processed: 5\n",
      "Average processing time: 6.29 seconds\n",
      "Average memory usage: 9.46 MB\n",
      "Average confidence score: 80.97%\n",
      "\n",
      "First results:\n",
      "          file_name  processing_time  memory_usage_mb  confidence_score  scientific_numbers  equations  tables  figures  avg_line_length  line_length_std  layout_consistency\n",
      "0  2411.04106v1.pdf         7.322845         8.667969         78.831950                   0          0       0        0        79.186667        26.673679            0.336846\n",
      "1  2411.04108v1.pdf         4.761900        13.238281         80.411255                   0          0       0        0        64.809524        31.935380            0.492758\n",
      "2  2411.04109v1.pdf         5.809524         8.382812         83.913374                   0          0       0        0        78.607843        30.133740            0.383343\n",
      "3  2411.04112v1.pdf         7.412029         8.320312         82.550813                   0          0       0        0       106.472727        23.382087            0.219606\n",
      "4  2411.04118v1.pdf         6.141591         8.667969         79.149051                   0          0       0        0        77.464286        24.489768            0.316143\n"
     ]
    }
   ],
   "source": [
    "class TesseractBenchmark:\n",
    "    def __init__(self, pdf_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize Tesseract OCR benchmarking tool.\n",
    "        \n",
    "        Args:\n",
    "            pdf_dir: Directory containing PDF files to process\n",
    "        \"\"\"\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.results = []\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging for the benchmark\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('tesseract_benchmark.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger('TesseractBenchmark')\n",
    "\n",
    "    def convert_pdf_to_images(self, pdf_path: Path) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Convert PDF pages to images.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            \n",
    "        Returns:\n",
    "            List of PIL Image objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Converting PDF to images: {pdf_path}\")\n",
    "            images = convert_from_path(str(pdf_path), dpi=300)\n",
    "            self.logger.info(f\"Successfully converted {len(images)} pages from {pdf_path}\")\n",
    "            return images\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error converting PDF {pdf_path}: {str(e)}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def process_image(self, image) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process image with Tesseract OCR and measure performance.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image object\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing OCR results and performance metrics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        try:\n",
    "            # Process with Tesseract\n",
    "            text = pytesseract.image_to_string(\n",
    "                image,\n",
    "                config='--psm 6 --oem 3'  # Page segmentation mode 6: Assume uniform text block\n",
    "            )\n",
    "            \n",
    "            # Get confidence scores\n",
    "            data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "            confidence_scores = [float(conf) for conf in data['conf'] if conf != '-1']\n",
    "            avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in OCR processing: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        end_time = time.time()\n",
    "        end_memory = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'processing_time': end_time - start_time,\n",
    "            'memory_usage': end_memory - start_memory,\n",
    "            'confidence': avg_confidence\n",
    "        }\n",
    "\n",
    "    def analyze_scientific_content(self, text: str) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Analyze scientific content in extracted text.\n",
    "        \n",
    "        Args:\n",
    "            text: Extracted text from OCR\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing counts of scientific elements\n",
    "        \"\"\"\n",
    "        # Scientific notation pattern (e.g., 1.23e-4)\n",
    "        scientific_pattern = r'\\d+\\.?\\d*[eE][+-]?\\d+'\n",
    "        \n",
    "        # Mathematical equation pattern (text between $ or $$)\n",
    "        equation_pattern = r'\\$.*?\\$|\\$\\$.*?\\$\\$'\n",
    "        \n",
    "        # Table header pattern\n",
    "        table_pattern = r'Table \\d+|TABLE \\d+'\n",
    "        \n",
    "        # Figure caption pattern\n",
    "        figure_pattern = r'Figure \\d+|Fig\\. \\d+|FIG\\. \\d+'\n",
    "        \n",
    "        return {\n",
    "            'scientific_numbers': len(re.findall(scientific_pattern, text)),\n",
    "            'equations': len(re.findall(equation_pattern, text)),\n",
    "            'tables': len(re.findall(table_pattern, text)),\n",
    "            'figures': len(re.findall(figure_pattern, text))\n",
    "        }\n",
    "\n",
    "    def analyze_layout(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze text layout characteristics.\n",
    "        \n",
    "        Args:\n",
    "            text: Extracted text from OCR\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing layout metrics\n",
    "        \"\"\"\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        \n",
    "        if not lines:\n",
    "            return {\n",
    "                'avg_line_length': 0,\n",
    "                'line_length_std': 0,\n",
    "                'layout_consistency': 0\n",
    "            }\n",
    "            \n",
    "        line_lengths = [len(line) for line in lines]\n",
    "        \n",
    "        return {\n",
    "            'avg_line_length': np.mean(line_lengths),\n",
    "            'line_length_std': np.std(line_lengths),\n",
    "            'layout_consistency': np.std(line_lengths) / np.mean(line_lengths) if np.mean(line_lengths) > 0 else 0\n",
    "        }\n",
    "\n",
    "    def run_benchmark(self, sample_size: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run the benchmark on PDF files.\n",
    "        \n",
    "        Args:\n",
    "            sample_size: Optional number of PDFs to process (for testing)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing benchmark results\n",
    "        \"\"\"\n",
    "        pdf_files = list(self.pdf_dir.glob('*.pdf'))\n",
    "        if sample_size:\n",
    "            pdf_files = pdf_files[:sample_size]\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            self.logger.info(f\"Processing {pdf_file.name}\")\n",
    "            \n",
    "            # Convert PDF to images\n",
    "            images = self.convert_pdf_to_images(pdf_file)\n",
    "            \n",
    "            if not images:\n",
    "                continue\n",
    "                \n",
    "            # Process first page only for benchmark\n",
    "            ocr_result = self.process_image(images[0])\n",
    "            \n",
    "            if ocr_result is None:\n",
    "                continue\n",
    "                \n",
    "            # Analyze the extracted text\n",
    "            scientific_metrics = self.analyze_scientific_content(ocr_result['text'])\n",
    "            layout_metrics = self.analyze_layout(ocr_result['text'])\n",
    "            \n",
    "            # Compile results\n",
    "            self.results.append({\n",
    "                'file_name': pdf_file.name,\n",
    "                'processing_time': ocr_result['processing_time'],\n",
    "                'memory_usage_mb': ocr_result['memory_usage'],\n",
    "                'confidence_score': ocr_result['confidence'],\n",
    "                **scientific_metrics,\n",
    "                **layout_metrics\n",
    "            })\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        \n",
    "        # Save results\n",
    "        results_df.to_csv('tesseract_benchmark_results.csv', index=False)\n",
    "        return results_df\n",
    "\n",
    "    def generate_report(self, results_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a summary report of the benchmark results.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame containing benchmark results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing summary statistics\n",
    "        \"\"\"\n",
    "        def safe_mean(series):\n",
    "            \"\"\"Calculate mean safely, return 0 if series is empty or doesn't exist\"\"\"\n",
    "            try:\n",
    "                return float(series.mean()) if not series.empty else 0\n",
    "            except:\n",
    "                return 0\n",
    "                \n",
    "        def safe_std(series):\n",
    "            \"\"\"Calculate standard deviation safely, return 0 if series is empty or doesn't exist\"\"\"\n",
    "            try:\n",
    "                return float(series.std()) if not series.empty else 0\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        report = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'total_files_processed': len(results_df),\n",
    "            'performance_metrics': {\n",
    "                'avg_processing_time': safe_mean(results_df.get('processing_time', pd.Series())),\n",
    "                'std_processing_time': safe_std(results_df.get('processing_time', pd.Series())),\n",
    "                'avg_memory_usage': safe_mean(results_df.get('memory_usage_mb', pd.Series())),\n",
    "                'avg_confidence': safe_mean(results_df.get('confidence_score', pd.Series()))\n",
    "            },\n",
    "            'content_metrics': {\n",
    "                'avg_scientific_numbers': safe_mean(results_df.get('scientific_numbers', pd.Series())),\n",
    "                'avg_equations': safe_mean(results_df.get('equations', pd.Series())),\n",
    "                'avg_tables': safe_mean(results_df.get('tables', pd.Series())),\n",
    "                'avg_figures': safe_mean(results_df.get('figures', pd.Series()))\n",
    "            },\n",
    "            'layout_metrics': {\n",
    "                'avg_layout_consistency': safe_mean(results_df.get('layout_consistency', pd.Series()))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save report\n",
    "        with open('tesseract_benchmark_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "            \n",
    "        return report\n",
    "\n",
    "def main():\n",
    "    # Set the correct path to PDFs\n",
    "    pdf_dir = r'C:\\Users\\thinkpad\\Documents\\GitHub\\Textra_AI_Research\\data\\raw\\pdfs'\n",
    "    \n",
    "    # Verify path exists\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        print(f\"Error: Directory {pdf_dir} does not exist!\")\n",
    "        return\n",
    "        \n",
    "    # Check if directory contains PDFs\n",
    "    pdf_files = list(Path(pdf_dir).glob('*.pdf'))\n",
    "    if not pdf_files:\n",
    "        print(f\"Error: No PDF files found in {pdf_dir}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(pdf_files)} PDF files in directory\")\n",
    "    \n",
    "    # Initialize benchmark\n",
    "    benchmark = TesseractBenchmark(pdf_dir=pdf_dir)\n",
    "    \n",
    "    try:\n",
    "        # Run benchmark on sample\n",
    "        print(\"Starting benchmark process...\")\n",
    "        results_df = benchmark.run_benchmark(sample_size=5)  # Start with 2 files for testing\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            print(\"No results were generated. Check the log file for errors.\")\n",
    "            return\n",
    "            \n",
    "        # Generate report\n",
    "        report = benchmark.generate_report(results_df)\n",
    "        \n",
    "        print(\"\\nBenchmark complete. Results saved to 'tesseract_benchmark_results.csv'\")\n",
    "        print(\"Detailed report saved to 'tesseract_benchmark_report.json'\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Files processed: {report['total_files_processed']}\")\n",
    "        print(f\"Average processing time: {report['performance_metrics']['avg_processing_time']:.2f} seconds\")\n",
    "        print(f\"Average memory usage: {report['performance_metrics']['avg_memory_usage']:.2f} MB\")\n",
    "        print(f\"Average confidence score: {report['performance_metrics']['avg_confidence']:.2f}%\")\n",
    "        \n",
    "        # Print first few results for verification\n",
    "        if not results_df.empty:\n",
    "            print(\"\\nFirst results:\")\n",
    "            print(results_df.head().to_string())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during benchmark: {str(e)}\")\n",
    "        logging.error(f\"Benchmark error: {str(e)}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import easyocr\n",
    "from pdf2image import convert_from_path\n",
    "import psutil\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.20 | packaged by conda-forge | (main, Sep 30 2024, 17:43:23) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas version: 2.2.3\n",
      "Numpy version: 2.0.2\n",
      "PyTorch version: 2.5.1+cpu\n",
      "CUDA available: False\n",
      "pdf2image imported successfully\n",
      "psutil version: 6.1.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"Pandas version:\", pd.__version__)\n",
    "    print(\"Numpy version:\", np.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"Error importing pandas/numpy:\", e)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "except ImportError as e:\n",
    "    print(\"Error importing PyTorch:\", e)\n",
    "\n",
    "try:\n",
    "    import pdf2image\n",
    "    print(\"pdf2image imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"Error importing pdf2image:\", e)\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "    print(\"psutil version:\", psutil.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"Error importing psutil:\", e)\n",
    "\n",
    "try:\n",
    "    import easyocr\n",
    "    print(\"EasyOCR imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"Error importing EasyOCR:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.20 | packaged by conda-forge | (main, Sep 30 2024, 17:43:23) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import easyocr\n",
    "    print(\"EasyOCR imported successfully\")\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    print(\"EasyOCR reader initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with EasyOCR: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class EasyOCRBenchmark:\n",
    "    def __init__(self, pdf_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize EasyOCR benchmarking tool.\n",
    "        \n",
    "        Args:\n",
    "            pdf_dir: Directory containing PDF files to process\n",
    "        \"\"\"\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.results = []\n",
    "        self.setup_logging()\n",
    "        self.setup_ocr()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging for the benchmark\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('easyocr_benchmark.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger('EasyOCRBenchmark')\n",
    "\n",
    "    def setup_ocr(self):\n",
    "        \"\"\"Initialize EasyOCR with desired settings\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Initializing EasyOCR...\")\n",
    "            self.reader = easyocr.Reader(['en'], gpu=False)  # Set gpu=True if GPU is available\n",
    "            self.logger.info(\"EasyOCR initialized successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing EasyOCR: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def convert_pdf_to_images(self, pdf_path: Path) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Convert PDF pages to images.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            \n",
    "        Returns:\n",
    "            List of PIL Image objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Converting PDF to images: {pdf_path}\")\n",
    "            images = convert_from_path(str(pdf_path), dpi=300)\n",
    "            self.logger.info(f\"Successfully converted {len(images)} pages from {pdf_path}\")\n",
    "            return images\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error converting PDF {pdf_path}: {str(e)}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def process_image(self, image) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process image with EasyOCR and measure performance.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image object\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing OCR results and performance metrics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        try:\n",
    "            # Convert PIL Image to numpy array if needed\n",
    "            image_np = np.array(image)\n",
    "            \n",
    "            # Process with EasyOCR\n",
    "            results = self.reader.readtext(image_np)\n",
    "            \n",
    "            # Extract text and confidence scores\n",
    "            text = ' '.join([result[1] for result in results])\n",
    "            confidence_scores = [result[2] for result in results]\n",
    "            avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in OCR processing: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        end_time = time.time()\n",
    "        end_memory = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'processing_time': end_time - start_time,\n",
    "            'memory_usage': end_memory - start_memory,\n",
    "            'confidence': avg_confidence,\n",
    "            'bounding_boxes': len(results)\n",
    "        }\n",
    "\n",
    "    def analyze_scientific_content(self, text: str) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Analyze scientific content in extracted text.\n",
    "        \n",
    "        Args:\n",
    "            text: Extracted text from OCR\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing counts of scientific elements\n",
    "        \"\"\"\n",
    "        # Scientific notation pattern (e.g., 1.23e-4)\n",
    "        scientific_pattern = r'\\d+\\.?\\d*[eE][+-]?\\d+'\n",
    "        \n",
    "        # Mathematical equation pattern (text between $ or $$)\n",
    "        equation_pattern = r'\\$.*?\\$|\\$\\$.*?\\$\\$'\n",
    "        \n",
    "        # Table header pattern\n",
    "        table_pattern = r'Table \\d+|TABLE \\d+'\n",
    "        \n",
    "        # Figure caption pattern\n",
    "        figure_pattern = r'Figure \\d+|Fig\\. \\d+|FIG\\. \\d+'\n",
    "        \n",
    "        # Reference pattern\n",
    "        reference_pattern = r'\\[\\d+\\]|\\[\\d+,\\s*\\d+\\]'\n",
    "        \n",
    "        return {\n",
    "            'scientific_numbers': len(re.findall(scientific_pattern, text)),\n",
    "            'equations': len(re.findall(equation_pattern, text)),\n",
    "            'tables': len(re.findall(table_pattern, text)),\n",
    "            'figures': len(re.findall(figure_pattern, text)),\n",
    "            'references': len(re.findall(reference_pattern, text))\n",
    "        }\n",
    "\n",
    "    def analyze_layout(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze text layout characteristics.\n",
    "        \n",
    "        Args:\n",
    "            text: Extracted text from OCR\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing layout metrics\n",
    "        \"\"\"\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        \n",
    "        if not lines:\n",
    "            return {\n",
    "                'avg_line_length': 0,\n",
    "                'line_length_std': 0,\n",
    "                'layout_consistency': 0\n",
    "            }\n",
    "            \n",
    "        line_lengths = [len(line) for line in lines]\n",
    "        \n",
    "        return {\n",
    "            'avg_line_length': np.mean(line_lengths),\n",
    "            'line_length_std': np.std(line_lengths),\n",
    "            'layout_consistency': np.std(line_lengths) / np.mean(line_lengths) if np.mean(line_lengths) > 0 else 0\n",
    "        }\n",
    "\n",
    "    def run_benchmark(self, sample_size: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run the benchmark on PDF files.\n",
    "        \n",
    "        Args:\n",
    "            sample_size: Optional number of PDFs to process (for testing)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing benchmark results\n",
    "        \"\"\"\n",
    "        pdf_files = list(self.pdf_dir.glob('*.pdf'))\n",
    "        if sample_size:\n",
    "            pdf_files = pdf_files[:sample_size]\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            self.logger.info(f\"Processing {pdf_file.name}\")\n",
    "            \n",
    "            # Convert PDF to images\n",
    "            images = self.convert_pdf_to_images(pdf_file)\n",
    "            \n",
    "            if not images:\n",
    "                continue\n",
    "                \n",
    "            # Process first page only for benchmark\n",
    "            ocr_result = self.process_image(images[0])\n",
    "            \n",
    "            if ocr_result is None:\n",
    "                continue\n",
    "                \n",
    "            # Analyze the extracted text\n",
    "            scientific_metrics = self.analyze_scientific_content(ocr_result['text'])\n",
    "            layout_metrics = self.analyze_layout(ocr_result['text'])\n",
    "            \n",
    "            # Compile results\n",
    "            self.results.append({\n",
    "                'file_name': pdf_file.name,\n",
    "                'processing_time': ocr_result['processing_time'],\n",
    "                'memory_usage_mb': ocr_result['memory_usage'],\n",
    "                'confidence_score': ocr_result['confidence'],\n",
    "                'bounding_boxes': ocr_result['bounding_boxes'],\n",
    "                **scientific_metrics,\n",
    "                **layout_metrics\n",
    "            })\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        \n",
    "        # Save results\n",
    "        results_df.to_csv('easyocr_benchmark_results.csv', index=False)\n",
    "        return results_df\n",
    "\n",
    "    def generate_report(self, results_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a summary report of the benchmark results.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame containing benchmark results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing summary statistics\n",
    "        \"\"\"\n",
    "        def safe_mean(series):\n",
    "            \"\"\"Calculate mean safely, return 0 if series is empty or doesn't exist\"\"\"\n",
    "            try:\n",
    "                return float(series.mean()) if not series.empty else 0\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        def safe_std(series):\n",
    "            \"\"\"Calculate standard deviation safely, return 0 if series is empty or doesn't exist\"\"\"\n",
    "            try:\n",
    "                return float(series.std()) if not series.empty else 0\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        report = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'total_files_processed': len(results_df),\n",
    "            'performance_metrics': {\n",
    "                'avg_processing_time': safe_mean(results_df.get('processing_time', pd.Series())),\n",
    "                'std_processing_time': safe_std(results_df.get('processing_time', pd.Series())),\n",
    "                'avg_memory_usage': safe_mean(results_df.get('memory_usage_mb', pd.Series())),\n",
    "                'avg_confidence': safe_mean(results_df.get('confidence_score', pd.Series())),\n",
    "                'avg_bounding_boxes': safe_mean(results_df.get('bounding_boxes', pd.Series()))\n",
    "            },\n",
    "            'content_metrics': {\n",
    "                'avg_scientific_numbers': safe_mean(results_df.get('scientific_numbers', pd.Series())),\n",
    "                'avg_equations': safe_mean(results_df.get('equations', pd.Series())),\n",
    "                'avg_tables': safe_mean(results_df.get('tables', pd.Series())),\n",
    "                'avg_figures': safe_mean(results_df.get('figures', pd.Series())),\n",
    "                'avg_references': safe_mean(results_df.get('references', pd.Series()))\n",
    "            },\n",
    "            'layout_metrics': {\n",
    "                'avg_layout_consistency': safe_mean(results_df.get('layout_consistency', pd.Series()))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save report\n",
    "        with open('easyocr_benchmark_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "            \n",
    "        return report\n",
    "\n",
    "def check_gpu_availability():\n",
    "    \"\"\"\n",
    "    Check if GPU is available for EasyOCR\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        return torch.cuda.is_available()\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Check GPU availability\n",
    "    gpu_available = check_gpu_availability()\n",
    "    print(f\"GPU Available: {gpu_available}\")\n",
    "    \n",
    "    # Set the correct path to PDFs\n",
    "    pdf_dir = r'data\\raw\\pdfs'\n",
    "    \n",
    "    # Verify path exists\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        print(f\"Error: Directory {pdf_dir} does not exist!\")\n",
    "        return\n",
    "        \n",
    "    # Check if directory contains PDFs\n",
    "    pdf_files = list(Path(pdf_dir).glob('*.pdf'))\n",
    "    if not pdf_files:\n",
    "        print(f\"Error: No PDF files found in {pdf_dir}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(pdf_files)} PDF files in directory\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize benchmark\n",
    "        benchmark = EasyOCRBenchmark(pdf_dir=pdf_dir)\n",
    "        \n",
    "        # Run benchmark on sample\n",
    "        print(\"Starting benchmark process...\")\n",
    "        results_df = benchmark.run_benchmark(sample_size=5)  # Start with 2 files for testing\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            print(\"No results were generated. Check the log file for errors.\")\n",
    "            return\n",
    "            \n",
    "        # Generate report\n",
    "        report = benchmark.generate_report(results_df)\n",
    "        \n",
    "        print(\"\\nBenchmark complete. Results saved to 'easyocr_benchmark_results.csv'\")\n",
    "        print(\"Detailed report saved to 'easyocr_benchmark_report.json'\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Files processed: {report['total_files_processed']}\")\n",
    "        print(f\"Average processing time: {report['performance_metrics']['avg_processing_time']:.2f} seconds\")\n",
    "        print(f\"Average memory usage: {report['performance_metrics']['avg_memory_usage']:.2f} MB\")\n",
    "        print(f\"Average confidence score: {report['performance_metrics']['avg_confidence']:.2f}%\")\n",
    "        print(f\"Average bounding boxes detected: {report['performance_metrics']['avg_bounding_boxes']:.0f}\")\n",
    "        \n",
    "        # Print first few results for verification\n",
    "        if not results_df.empty:\n",
    "            print(\"\\nFirst results:\")\n",
    "            print(results_df.head().to_string())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during benchmark: {str(e)}\")\n",
    "        logging.error(f\"Benchmark error: {str(e)}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paddleocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.20 | packaged by conda-forge | (main, Sep 30 2024, 17:43:23) [MSC v.1929 64 bit (AMD64)]\n",
      "Error with PaddleOCR: No module named 'paddle'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    from paddleocr import PaddleOCR\n",
    "    print(\"PaddleOCR imported successfully\")\n",
    "    # Initialize PaddleOCR with minimal settings\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=False)\n",
    "    print(\"PaddleOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with PaddleOCR: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from paddleocr import PaddleOCR\n",
    "from pdf2image import convert_from_path\n",
    "import psutil\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "class PaddleOCRBenchmark:\n",
    "    def __init__(self, pdf_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize PaddleOCR benchmarking tool.\n",
    "        \n",
    "        Args:\n",
    "            pdf_dir: Directory containing PDF files to process\n",
    "        \"\"\"\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.results = []\n",
    "        self.setup_logging()\n",
    "        self.setup_ocr()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging for the benchmark\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('paddleocr_benchmark.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger('PaddleOCRBenchmark')\n",
    "\n",
    "    def setup_ocr(self):\n",
    "        \"\"\"Initialize PaddleOCR with desired settings\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Initializing PaddleOCR...\")\n",
    "            self.ocr = PaddleOCR(\n",
    "                use_angle_cls=True,  # Detect text orientation\n",
    "                lang='en',          # English language\n",
    "                use_gpu=False,      # CPU only\n",
    "                show_log=False,     # Disable verbose logging\n",
    "                enable_mkldnn=True  # Enable Intel MKL optimization\n",
    "            )\n",
    "            self.logger.info(\"PaddleOCR initialized successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing PaddleOCR: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def convert_pdf_to_images(self, pdf_path: Path) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Convert PDF pages to images.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "        Returns:\n",
    "            List of PIL Image objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Converting PDF to images: {pdf_path}\")\n",
    "            images = convert_from_path(\n",
    "                str(pdf_path), \n",
    "                dpi=300,\n",
    "                first_page=1,\n",
    "                last_page=1  # Only process first page for benchmark\n",
    "            )\n",
    "            self.logger.info(f\"Successfully converted {len(images)} pages from {pdf_path}\")\n",
    "            return images\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error converting PDF {pdf_path}: {str(e)}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def process_image(self, image) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a single image with PaddleOCR and measure performance.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image object\n",
    "        Returns:\n",
    "            Dictionary containing OCR results and performance metrics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        try:\n",
    "            # Save image temporarily (PaddleOCR works better with file input)\n",
    "            temp_path = \"temp_page.jpg\"\n",
    "            image.save(temp_path)\n",
    "            \n",
    "            # Process with PaddleOCR\n",
    "            result = self.ocr.ocr(temp_path)\n",
    "            \n",
    "            # Extract text and confidence scores\n",
    "            texts = []\n",
    "            confidence_scores = []\n",
    "            \n",
    "            if result[0]:  # Check if any text was detected\n",
    "                for line in result[0]:\n",
    "                    if len(line) >= 2:  # Check if line contains both bbox/text and confidence\n",
    "                        texts.append(line[1][0])  # Text content\n",
    "                        confidence_scores.append(float(line[1][1]))  # Confidence score\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            os.remove(temp_path)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            end_time = time.time()\n",
    "            end_memory = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "            \n",
    "            return {\n",
    "                'text': ' '.join(texts),\n",
    "                'processing_time': end_time - start_time,\n",
    "                'memory_usage': end_memory - start_memory,\n",
    "                'confidence': np.mean(confidence_scores) if confidence_scores else 0,\n",
    "                'text_blocks': len(texts)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in OCR processing: {str(e)}\")\n",
    "            if os.path.exists(\"temp_page.jpg\"):\n",
    "                os.remove(\"temp_page.jpg\")\n",
    "            return None\n",
    "\n",
    "    def analyze_content(self, text: str) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Analyze scientific content in extracted text.\n",
    "        \n",
    "        Args:\n",
    "            text: Extracted text from OCR\n",
    "        Returns:\n",
    "            Dictionary containing content analysis metrics\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Scientific notation pattern\n",
    "        scientific_pattern = r'\\d+\\.?\\d*[eE][+-]?\\d+'\n",
    "        \n",
    "        # Table pattern\n",
    "        table_pattern = r'Table \\d+|TABLE \\d+'\n",
    "        \n",
    "        # Figure pattern\n",
    "        figure_pattern = r'Figure \\d+|Fig\\. \\d+|FIG\\. \\d+'\n",
    "        \n",
    "        # Reference pattern\n",
    "        reference_pattern = r'\\[\\d+\\]|\\[\\d+,\\s*\\d+\\]'\n",
    "        \n",
    "        return {\n",
    "            'scientific_numbers': len(re.findall(scientific_pattern, text)),\n",
    "            'tables': len(re.findall(table_pattern, text)),\n",
    "            'figures': len(re.findall(figure_pattern, text)),\n",
    "            'references': len(re.findall(reference_pattern, text))\n",
    "        }\n",
    "\n",
    "    def run_benchmark(self, sample_size: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run the benchmark on PDF files.\n",
    "        \n",
    "        Args:\n",
    "            sample_size: Optional number of PDFs to process\n",
    "        Returns:\n",
    "            DataFrame containing benchmark results\n",
    "        \"\"\"\n",
    "        pdf_files = list(self.pdf_dir.glob('*.pdf'))\n",
    "        if sample_size:\n",
    "            pdf_files = pdf_files[:sample_size]\n",
    "\n",
    "        self.logger.info(f\"Starting benchmark with {len(pdf_files)} files\")\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            self.logger.info(f\"Processing {pdf_file.name}\")\n",
    "            \n",
    "            # Convert PDF to images\n",
    "            images = self.convert_pdf_to_images(pdf_file)\n",
    "            \n",
    "            if not images:\n",
    "                continue\n",
    "                \n",
    "            # Process first page\n",
    "            ocr_result = self.process_image(images[0])\n",
    "            \n",
    "            if ocr_result is None:\n",
    "                continue\n",
    "                \n",
    "            # Analyze the extracted text\n",
    "            content_metrics = self.analyze_content(ocr_result['text'])\n",
    "            \n",
    "            # Compile results\n",
    "            self.results.append({\n",
    "                'file_name': pdf_file.name,\n",
    "                'processing_time': ocr_result['processing_time'],\n",
    "                'memory_usage_mb': ocr_result['memory_usage'],\n",
    "                'confidence_score': ocr_result['confidence'],\n",
    "                'text_blocks': ocr_result['text_blocks'],\n",
    "                **content_metrics\n",
    "            })\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        \n",
    "        # Save results\n",
    "        results_df.to_csv('paddleocr_benchmark_results.csv', index=False)\n",
    "        return results_df\n",
    "\n",
    "    def generate_report(self, results_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a summary report of the benchmark results.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame containing benchmark results\n",
    "        Returns:\n",
    "            Dictionary containing summary statistics\n",
    "        \"\"\"\n",
    "        def safe_mean(series):\n",
    "            \"\"\"Calculate mean safely, return 0 if series is empty or doesn't exist\"\"\"\n",
    "            try:\n",
    "                return float(series.mean()) if not series.empty else 0\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        report = {\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'total_files_processed': len(results_df),\n",
    "            'performance_metrics': {\n",
    "                'avg_processing_time': safe_mean(results_df.get('processing_time', pd.Series())),\n",
    "                'avg_memory_usage': safe_mean(results_df.get('memory_usage_mb', pd.Series())),\n",
    "                'avg_confidence': safe_mean(results_df.get('confidence_score', pd.Series())),\n",
    "                'avg_text_blocks': safe_mean(results_df.get('text_blocks', pd.Series()))\n",
    "            },\n",
    "            'content_metrics': {\n",
    "                'avg_scientific_numbers': safe_mean(results_df.get('scientific_numbers', pd.Series())),\n",
    "                'avg_tables': safe_mean(results_df.get('tables', pd.Series())),\n",
    "                'avg_figures': safe_mean(results_df.get('figures', pd.Series())),\n",
    "                'avg_references': safe_mean(results_df.get('references', pd.Series()))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save report\n",
    "        with open('paddleocr_benchmark_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "            \n",
    "        return report\n",
    "\n",
    "def main():\n",
    "    # Set the correct path to PDFs\n",
    "    pdf_dir = r'data\\raw\\pdfs'\n",
    "    \n",
    "    # Verify path exists\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        print(f\"Error: Directory {pdf_dir} does not exist!\")\n",
    "        return\n",
    "        \n",
    "    # Check if directory contains PDFs\n",
    "    pdf_files = list(Path(pdf_dir).glob('*.pdf'))\n",
    "    if not pdf_files:\n",
    "        print(f\"Error: No PDF files found in {pdf_dir}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(pdf_files)} PDF files in directory\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize benchmark\n",
    "        benchmark = PaddleOCRBenchmark(pdf_dir=pdf_dir)\n",
    "        \n",
    "        # Run benchmark on sample\n",
    "        print(\"Starting benchmark process...\")\n",
    "        results_df = benchmark.run_benchmark(sample_size=1)  # Start with 1 file for testing\n",
    "        \n",
    "        # Generate report\n",
    "        report = benchmark.generate_report(results_df)\n",
    "        \n",
    "        print(\"\\nBenchmark complete. Results saved to 'paddleocr_benchmark_results.csv'\")\n",
    "        print(\"Detailed report saved to 'paddleocr_benchmark_report.json'\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Files processed: {report['total_files_processed']}\")\n",
    "        print(f\"Average processing time: {report['performance_metrics']['avg_processing_time']:.2f} seconds\")\n",
    "        print(f\"Average memory usage: {report['performance_metrics']['avg_memory_usage']:.2f} MB\")\n",
    "        print(f\"Average confidence score: {report['performance_metrics']['avg_confidence']:.2f}%\")\n",
    "        print(f\"Average text blocks detected: {report['performance_metrics']['avg_text_blocks']:.0f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during benchmark: {str(e)}\")\n",
    "        logging.error(f\"Benchmark error: {str(e)}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
